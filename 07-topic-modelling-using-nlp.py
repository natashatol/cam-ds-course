# -*- coding: utf-8 -*-
"""07-topic-modelling-using-NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/108Ov9PlvsAtmm_fqjYV49cHKuWJqbVrK

# Topic project 4.1 Applying NLP for topic modelling in a real-life context
"""

!pip install bertopic

!pip install gensim pyLDAvis

#import necessary libraries
import pandas as pd
import numpy as np
import nltk
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
import random
import json
import torch
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

from nltk.corpus import stopwords
from hdbscan import HDBSCAN
from google.colab import files
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from bertopic import BERTopic
from transformers import pipeline
from nltk.stem.wordnet import WordNetLemmatizer
from gensim import corpora
from gensim.models.ldamodel import LdaModel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction import text

nltk.download('stopwords')
nltk.download('punkt')#for tokenisation
nltk.download('punkt_tab')#for tokenisation

"""##Data Exploration and Cleaning"""

#import data
df_google = pd.read_excel('Google_12_months.xlsx')
df_trustpilot = pd.read_excel('Trustpilot_12_months.xlsx')

#view google data
df_google.head(2)

#view trustpilot
df_trustpilot.head(2)

#remove unecessary columns from trustpilot reviews
df_trustpilot.drop(columns=['Review ID','Review Consumer User ID','Source Of Review','Domain URL','Webshop Name','Business Unit ID','Tags','Location ID'],inplace=True)
df_trustpilot.sample(2)

#check data types and nulls
df_trustpilot.info()

#remove unecessary google data columns
df_google.drop(columns=['Customer Name','SurveyID for external use (e.g. tech support)','Social Media Source'],axis=1,inplace=True)
df_google.head(2)

#check data types and nulls
df_google.info()

#drop rows with null data
df_google.dropna(subset=['Comment'],inplace=True)
df_trustpilot.dropna(subset=['Review Content'],inplace=True)

#drop non english reviews in trustpilot
df_trustpilot = df_trustpilot[df_trustpilot['Review Language']=='en'].drop(columns=['Review Language'],axis=1)

#rename columns for easier use
df_google=df_google.rename(columns={"Club's Name":'location',
                          "Creation Date":'review_date',
                          "Comment":'review',
                          "Overall Score":'score'
                          })
df_trustpilot=df_trustpilot.rename(columns={"Review Created (UTC)":'review_date',
                          "Review Title":'title',
                          "Review Content":'review',
                          "Review Stars":'score',
                          "Company Reply Date (UTC)":'reply_date',
                          "Location Name":'location'
                          })

print('Google Locations:' ,df_google['location'].nunique())
print('Trustpilot Locations:',df_trustpilot['location'].nunique())
common_locations = set(df_google['location']).intersection(set(df_trustpilot['location']))
print('Common locations:',len(common_locations))

#extract negative reviews only
neg_google_reviews = df_google[df_google["score"] < 3].copy()
neg_trustpilot_reviews = df_trustpilot[df_trustpilot["score"] < 3].copy()

"""##Data Preprocessing"""

def preprocess_text(df,col):
    #remove html tags
    df[col] = df[col].apply(lambda x: re.sub(r'<.*?>', '', x))
    #convert to lower case and remove numbers
    reviews = df[col].str.lower().str.replace(r'\d+','',regex=True).str.cat(sep=' ')
    #remove punctuation.
    reviews = reviews.translate(str.maketrans('', '', string.punctuation))
    #tokenise the words
    word_tokens = word_tokenize(reviews)
    #set and remove stop words
    stop_words = set(stopwords.words('english'))
    #add gym to stop words as add no value
    custom_stop_words = stop_words.union({'gym'})
    filtered_tokens = [w for w in word_tokens if not w.lower() in custom_stop_words]
    #remove short words
    filtered_tokens = [w for w in filtered_tokens if len(w) > 2]

    return filtered_tokens

google_tokens = preprocess_text(df_google,'review')
trustpilot_tokens = preprocess_text(df_trustpilot,'review')

"""##Initial Data Investigation"""

def freq_dist(tokens):
    dist = nltk.FreqDist(tokens)
    freq_df = pd.DataFrame(dist.most_common(10),
                    columns=['Word', 'Frequency'])
    plt.figure(figsize=(10, 5))
    sns.barplot(x="Word", y="Frequency", data=freq_df)
    plt.tight_layout()


def wordcloud(tokens):
    wc = WordCloud(
        background_color='black',
        max_words=1000,
        max_font_size=50,
        width=800,
        height=400,
        random_state=42
    ).generate(' '.join(tokens))
    plt.figure(figsize=(10, 10), dpi=150)  # smaller size, higher dpi
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')

#plot top 10 words in google reviews
freq_dist(google_tokens)
plt.title('Fig 1A - Top 10 Words in Google Reviews')
#plot top 10 words in trustpilot reviews
freq_dist(trustpilot_tokens)
plt.title('Fig 1B - Top 10 Words in Trustpilot Reviews')

#wordcloud for google reviews
wordcloud(google_tokens)
plt.title('Fig 1C - Google Reviews - Word Cloud')
#wordcloud for trustpilot reviews
wordcloud(trustpilot_tokens)
plt.title('Fig 1D - Trustpilot Reviews - Word Cloud')

#repeat process above on the negative reviews only
neg_google_tokens = preprocess_text(neg_google_reviews,'review')
neg_trustpilot_tokens = preprocess_text(neg_trustpilot_reviews,'review')

freq_dist(neg_google_tokens)
plt.title('Fig 2A - Negative Google Reviews - Top 10 Words')
freq_dist(neg_trustpilot_tokens)
plt.title('Fig 2B - Negative Trustpilot Reviews - Top 10 Words')

wordcloud(neg_google_tokens)
plt.title('Fig 2C - Negative Google Reviews - Word Cloud')
wordcloud(neg_trustpilot_tokens)
plt.title('Fig 2D - Negative Trustpilot Reviews - Word Cloud')

"""##Conducting Initial Topic Modelling"""

#filter to only common locations
df_neg_google_common_loc = neg_google_reviews[neg_google_reviews['location'].isin(common_locations)].astype(str)
neg_google_common_loc_list = df_neg_google_common_loc['review'].tolist()
df_neg_trustpilot_common_loc= neg_trustpilot_reviews[neg_trustpilot_reviews['location'].isin(common_locations)].astype(str)
neg_trustpilot_common_loc_list= df_neg_trustpilot_common_loc['review'].tolist()
#merge reviews
all_neg_common_loc_df=pd.concat([df_neg_google_common_loc['review'],df_neg_trustpilot_common_loc['review']]).to_frame(name='review').astype(str)
all_neg_common_loc_list = neg_google_common_loc_list + neg_trustpilot_common_loc_list

def preprocess_bert(review_list, extra_stopwords=None):
    stop_words = set(stopwords.words('english'))
    if extra_stopwords:
        stop_words = stop_words.union(set(extra_stopwords))

    cleaned_reviews = []
    for review in review_list:
        review = review.lower() #convert to lower case
        review = re.sub(r'[^\w\s]', '', review)  # remove punctuation
        review = re.sub(r'\d+', '', review)      # remove digits

        tokens = word_tokenize(review) #tokenise the data
        filtered_tokens = [w for w in tokens if len(w) > 2 and w not in stop_words]

        cleaned_review = ' '.join(filtered_tokens)
        cleaned_reviews.append(cleaned_review)

    return cleaned_reviews
all_neg_reviews_clean = preprocess_bert(all_neg_common_loc_list, extra_stopwords={'gym'})
print(all_neg_reviews_clean[:2])

#run BERTopic using count vectorizer
model = BERTopic(verbose=True)
model.fit(all_neg_reviews_clean)
topic, probabilities = model.transform(all_neg_reviews_clean)

#print out top topics and document frequencies
model.get_topic_freq().head(10)

#print out top words for first topic
model.get_topic(0)

#print out top words for second topic
model.get_topic(1)

#create a visualisation for the cluster of topics
fig = model.visualize_topics()
fig.update_layout(
    title={
        'text': 'Fig 3C: Topic Clusters',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': {'size': 20}
    }
)
fig.show()

#view top 5 words for topics
fig = model.visualize_barchart()
fig.update_layout(
    title={
        'text': 'Fig 3A: Top 5 Words per Topic in Negative Reviews',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': {'size': 20}
    }
)
fig.show()

"""Top topics and their themes:
*   Topic 0: Equipment problems - condition of gym equipment, especially weights and machines.
*   Topic 1: Air conditioning - suggests uncomfortable temperatures.
*   Topic 2: Classes - complaints about availability and cancellations.
*   Topic 3: Parking - issues around parking convenience and/or charges.
*   Topic 4: Opening hours - concerns about closures.
*   Topic 5: Showers - suggesting water temperature is not hot enough.
*   Topic 6: Cleanliness - particularly related to toilets and changing rooms.
*   Topic 7: Membership problems - presumably difficulty cancelling/suspending.

"""

#view similarity matrix
fig = model.visualize_heatmap()
fig.update_layout(
    title={
        'text': 'Fig 3B: Topic Similarity Heatmap',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': {'size': 20}
    }
)
fig.show()

"""##Performing further data investigation"""

#most negative google reveiew locations
neg_google_reviews.groupby('location')['review'].count().sort_values(ascending=False).head(20).reset_index()

#most negative trustpilot reveiew locations
neg_trustpilot_reviews.groupby('location')['review'].count().sort_values(ascending=False).head(20).reset_index()

"""There is some overlap in locations, and a large majority of negative reviews are within the london area. However, they likely have a higher number of members. However, 2 of the top locations in the google reviews are London Canary Wharf and London Woolwich, which do not appear in the trustpilot sample."""

#merge the datasets to see the total number of negative reviews across trustpilot and google
neg_google_review_loc_count = neg_google_reviews.groupby('location')['review'].count().sort_values(ascending=False)
neg_trustpilot_review_loc_count = neg_trustpilot_reviews.groupby('location')['review'].count().sort_values(ascending=False)
neg_review_count = pd.concat([neg_google_review_loc_count,neg_trustpilot_review_loc_count],axis=1).fillna(0).reset_index()
neg_review_count.columns = ['Location', 'Google', 'Trustpilot']
neg_review_count['Total']=neg_review_count['Google']+neg_review_count['Trustpilot']
neg_review_count.sort_values('Total',ascending=False,inplace=True)
neg_review_count

import matplotlib.pyplot as plt

# Select top 30 negative reviews
neg_review_count_report = neg_review_count.head(30)

# Adjust figure size based on row count
row_count = len(neg_review_count_report)
fig_height = max(1, row_count * 0.4)
fig, ax = plt.subplots(figsize=(10, fig_height + 1))  # Add extra space for title

# Turn off axes
ax.axis('off')

# Create table
table = ax.table(
    cellText=neg_review_count_report.values,
    colLabels=neg_review_count_report.columns,
    loc='center',
    cellLoc='center'
)

# Style
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 1.5)

# Make column headers bold
for (row, col), cell in table.get_celld().items():
    if row == 0:
        cell.set_text_props(weight='bold')

# Add title
plt.title("Fig 4A - Top 30 Locations by Total Negative Reviews", fontsize=14, weight='bold')

# Save with tight bounding box
fig.savefig("negative_review_table.png", dpi=300, bbox_inches='tight')
plt.show()

#recalculate word cloud and frequency for top 30 negative lovations
top_neg_review_locs = neg_review_count['Location'].head(30)
top_neg_review_locs

#extract reviews in top 30 worst locations only
df_neg_trustpilot_worst_loc= neg_trustpilot_reviews[neg_trustpilot_reviews['location'].isin(top_neg_review_locs)].astype(str)
df_neg_google_worst_loc = neg_google_reviews[neg_google_reviews['location'].isin(top_neg_review_locs)].astype(str)

#preprocess data and run for top 30 worst locations
#plot top 10 words in google reviews
freq_dist(preprocess_text(df_neg_google_worst_loc,'review'))
plt.title('Top 10 Words in Google Reviews - Worst Locations')
#plot top 10 words in trustpilot reviews
freq_dist(preprocess_text(df_neg_trustpilot_worst_loc,'review'))
plt.title('Top 10 Words in Trustpilot Reviews - Worst Locations')

wordcloud(preprocess_text(df_neg_google_worst_loc,'review'))
plt.title('Negative Google Reviews - Word Cloud')
wordcloud(preprocess_text(df_neg_trustpilot_worst_loc,'review'))
plt.title('Negative Trustpilot Reviews - Word Cloud')

"""Results are not widely dissimilar from the initial results, particularly in the word frequency distribution, equipment, machines and staff are understandably common across all. The word clouds show more prominent negative words such as dirty, bad, broken, smell and rude."""

#combine results and run through BERTopic again
all_neg_worst_loc_df=pd.concat([df_neg_google_worst_loc['review'],df_neg_trustpilot_worst_loc['review']]).to_frame(name='review').astype(str)
neg_trustpilot_worst_loc_list = df_neg_trustpilot_worst_loc['review'].tolist()
neg_google_worst_loc_list = df_neg_google_worst_loc['review'].tolist()
all_neg_worst_loc_list = neg_trustpilot_worst_loc_list + neg_google_worst_loc_list

#preprocess the data using function created earlier for BERTopic
all_neg_reviews_worst_loc_clean = preprocess_bert(all_neg_worst_loc_list, extra_stopwords={ 'gym'})
all_neg_reviews_worst_loc_clean[:2]

#run BERTopic using preprocessed data
model_worst_loc = BERTopic(verbose=True)
model_worst_loc.fit(all_neg_reviews_worst_loc_clean)
topic, probabilities = model_worst_loc.transform(all_neg_reviews_worst_loc_clean)

#print out top topics
model_worst_loc.get_topic_freq().head(10)

#print out top words for first topic
model_worst_loc.get_topic(0)

#print out top words for second topic
model_worst_loc.get_topic(1)

#create a visualisation for the cluster of topics
fig = model_worst_loc.visualize_topics()
fig.update_layout(
    title={
        'text': 'Fig 4C: Topic Clusters - Top 30 Locations',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': {'size': 20}
    }
)
fig.show()

#view top 5 words for topics
fig = model_worst_loc.visualize_barchart()
fig.show()

#view similarity matrix
fig = model_worst_loc.visualize_heatmap()
fig.update_layout(
    title={
        'text': 'Fig 4B: Topic Similarity Heatmap - Top 30',
        'x': 0.5,
        'xanchor': 'center',
        'yanchor': 'top',
        'font': {'size': 20}
    }
)

#make x-axis labels horizontal
fig.update_xaxes(tickangle=90)

fig.show()

"""There are much fewer outliers in this second run of BERTopic. The topics listed are very similar, though in differing order of importance. One topic that comes much higher up this time was around staff/management and rudeness. Another topic is what I presume to be access/contact problems in topic 4, where words such as pin, code and pass appear frequently.

BERTopic recquires a large amount of data for optimal performance, therefore when initially running BERTopic for the top 30 worst locations it only produced two topics, with 98% in one. It needed fine tuning in order to create coherent topics, which suggests maybe there was not enough data.

##Emotion Analysis
"""

#import BERT model bhadresh-savani/bert-base-uncased-emotion from hugging face and set up a pipeline for text classification
classifier = pipeline("text-classification", model='bhadresh-savani/bert-base-uncased-emotion', return_all_scores=True)

#run a test prompt and see emotion classifications
test_prompt = "I love to hate the gym"
emotion_labels = classifier(test_prompt, )
emotion_labels_sorted = sorted(emotion_labels[0], key=lambda x: x["score"], reverse=True)
print(emotion_labels_sorted)

#find out top emotion for each review in google and trustpilot datasets
top_emotion = pipeline(
    "text-classification",
    model="bhadresh-savani/bert-base-uncased-emotion",
    return_all_scores=False , # only get the top label,
    truncation =True # running initially without did not work as tokens were too long
)

# apply to each review in the trustpilot dataset
df_trustpilot['top_emotion'] = df_trustpilot['review'].apply(lambda x: top_emotion(x)[0]['label'])

# apply to each review in the google dataset
df_google['top_emotion'] = df_google['review'].apply(lambda x: top_emotion(x)[0]['label'])

#filter dataset to negative reviews only and extract top emotion
combined_emotions = pd.concat([
    df_trustpilot[df_trustpilot['score']<3]['top_emotion'],
    df_google[df_google['score']<3]['top_emotion']
    ])

combined_emotions = combined_emotions.dropna()
#find counts of each emotion of negative reviews
emotion_counts = combined_emotions.value_counts().sort_values(ascending=False)
emotion_counts

# plot bar chart for top emotion distribution
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.barplot(x=emotion_counts.index, y=emotion_counts.values, palette="Set2")

plt.title("Top Emotion Distribution in Negative Reviews", fontsize=16)
plt.xlabel("Emotion", fontsize=12)
plt.ylabel("Number of Reviews", fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#get negative reviews from google and trustpilot where top review is anger
anger_reviews_trustpilot = df_trustpilot[(df_trustpilot['top_emotion']=='anger')&(df_trustpilot['score']<3)]['review']
anger_reviews_google = df_google[(df_google['top_emotion']=='anger')&(df_google['score']<3)]['review']

#preprocess data as before to be able to run BERTopic on negative angry reviews
anger_reviews_trustpilot_list = anger_reviews_trustpilot.tolist()
anger_reviews_google_list = anger_reviews_google.tolist()
all_anger_reviews_list = anger_reviews_trustpilot_list + anger_reviews_google_list

#preprocess the data using function created earlier for BERTopic
all_anger_reviews_clean = preprocess_bert(all_anger_reviews_list, extra_stopwords={'gym'})
all_anger_reviews_clean[:2]

#run BERTopic using count vectorizer
anger_model = BERTopic(verbose=True)
anger_model.fit(all_anger_reviews_clean)
topic, probabilities = anger_model.transform(all_anger_reviews_clean)

#create a visualisation for the cluster of topics
anger_model.visualize_topics()

"""There are multiple clusters of topics, with a few large circles indicating dominant topics. The large topic includes all german words, which suggests these should either be translated or removed from analysis.

##Using an LLM from Hugging Face

For this section I switch to use GPU
"""

!pip install -q transformers accelerate einops  #install recquired packages

#set random seed for reproducable results
torch.random.manual_seed(0)

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

#load model with GPU support
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    device_map="cuda", #load across available GPUs
    trust_remote_code=True,
    torch_dtype="auto"
)
#load tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")

#text generation pipeline with a 1000-token output limit
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=300, #had to reduce from 1000 due to memory usage
    device_map="auto"
)

generation_args = {
    "max_new_tokens": 300, #limits the output to 300 tokens
    "return_full_text": False, #return only the topics, not the prompt as well
    "do_sample": False, #removes randomness
}

#filter to only common locations, as full negative review list did not work on free google colab
df_neg_google_common_loc = neg_google_reviews[neg_google_reviews['location'].isin(common_locations)].astype(str)
neg_google_common_loc_list = df_neg_google_common_loc['review'].tolist()
df_neg_trustpilot_common_loc= neg_trustpilot_reviews[neg_trustpilot_reviews['location'].isin(common_locations)].astype(str)
neg_trustpilot_common_loc_list= df_neg_trustpilot_common_loc['review'].tolist()
#merge reviews
all_neg_common_loc_df=pd.concat([df_neg_google_common_loc['review'],df_neg_trustpilot_common_loc['review']]).to_frame(name='review').astype(str)
all_neg_common_loc_list = neg_google_common_loc_list + neg_trustpilot_common_loc_list
neg_list = []
for review in all_neg_common_loc_list:
  if review is not None:
    neg_list.append(review)

#loop through all negative reviews, where less than 500 tokens
topics_llm = []
iter = 0
for review in random.sample(neg_list, 400): #had to use a sample due to memory usage
  if len(str(review)) < 500:
    print("review -> ", review)
    #first of all want to list the top 3 topics per each negative review
    messages_2 = [
      {"role": "system", "content": "You work as a data analyst insights guru for a top online gym company and you want to find topics for improvments from customer reviews. You should return these in an array of strings only ['topic 1', 'topic 2', 'topic 3']"},
      {"role": "user", "content": f"In the following customer review interaction pick out the main 3 topics and return them as an array of topics: {review}"},
    ]
    output = pipe(messages_2, **generation_args)

    #create a list
    try:
        topic_list_string = output[0]['generated_text'].replace("'", '"')
        topic_list = json.loads(topic_list_string)
        topics_llm.append(topic_list)
    except Exception as e:
        continue
    torch.cuda.empty_cache() #memory crashes further down so trying to reduce memory usage

#pad sequences to make sure they are uniform in length
max_len = max(len(seq) for seq in topics_llm)
padded_topics_llm = [seq + [""] * (max_len - len(seq)) for seq in topics_llm]
print(padded_topics_llm)
#save and reload for later use
np_topics_llm = np.array(padded_topics_llm)
np.save('np_topics_phi.npy', np_topics_llm)
loaded_arr = np.load('np_topics_phi.npy')
topics_llm = loaded_arr.tolist()
#flatten 2d list into 1d and remove empty strings
topic_string_array = [item for row in topics_llm for item in row if item.strip()]

#condense all the topics from the pipeline above to get a numbered list
messages_3 = [
    {"role": "system", "content": "You work as a data analyst insights guru for a large online gym company and you want to compress a given list into collated topics. You should return these in a numbered list as business insights that can be used to improve the business"},
    {"role": "user", "content": f"In the following list containing the main extracted topics from customer reviews, group or compress the topics and return them with actionable insights in a numbered list: {topic_string_array}"},
]
output = pipe(messages_3, **generation_args)

#generate the insights using output from pipeline
insights = output[0]['generated_text']

print(insights)

#use BERTopic to see how results differ using the LLM vs previous runs to see if there are more actionable insights
model = BERTopic(verbose=True)
model.fit(topic_string_array)
topic, probabilities = model.transform(topic_string_array)

#print out top topics
model.get_topic_freq().head(10)

#print out top words for first topic
model.get_topic(0)

#print out top words for second topic
model.get_topic(1)

#create a visualisation for the cluster of topics
model.visualize_topics()

#view top 5 words for topics
fig = model.visualize_barchart()
fig.show()

#view similarity matrix
model.visualize_heatmap()

"""Given the small sample size, it may not be representative of the whole dataset, but we can see new themese arising that we have not seen before, for example covid19, and safety concerns. High similarity scores remain, including overlapping in the intertopic distance plot.

##Using Gensim
"""

# download necessary NLTK data.
nltk.download('stopwords')
nltk.download('wordnet')

# define stop words and punctuation.
stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()

#function to clean the document.
def clean(review):
    # Remove stop words and convert to lowercase.
    stop_free = " ".join([word for word in review.lower().split() if word not in stop])
    # Remove punctuation.
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    # Lemmatise the text.
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized

#clean all the negative reviews
neg_list = [clean(review) for review in neg_list]
#tokenise negative review list and convert to lower case
tokenized_neg_list = [review.lower().split() for review in neg_list]

#create a dictionary representation of the reviews
dictionary = corpora.Dictionary(tokenized_neg_list)

# filter out words that occur fewer than 2 reviews or more than 50% of the reviews.
dictionary.filter_extremes(no_below=2, no_above=0.5)

# create a BOW representation of the reviews
corpus = [dictionary.doc2bow(doc) for doc in tokenized_neg_list]

# set parameters.
num_topics = 10
passes = 20 #number of epochs for training

# create the LDA model.
lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=passes)

# print the topics.
for idx, topic in lda_model.print_topics(-1):
    print("Topic: {} \nWords: {}".format(idx, topic))

# create the visualisation.
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)